---
title: "Disaster Tweet Prediction"
author: "Anvil"
date: "08/10/2020"
output: 
   html_document:
      code_folding: show
      keep_md : true
      number_sections: true
      toc: true
      toc_depth: 3
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = F, message = F)
```

# Summary



# Loading packages

Throughout this project, we will use the following packages:


```{r}
library(ggplot2)
library(ggrepel)
library(gridExtra)
library(stringi)
library(stringr)
library(dplyr)
library(data.table)
library(quanteda)
library(quanteda.textmodels)
library(RColorBrewer)
library(irlba)
library(caret)
library(randomForest)
library(gbm)
library(xgboost)
library(e1071)
```

# Loading the data

As a personal habit, I automate the downloading and loading processes. The original data can be found on [this Kaggle page](https://www.kaggle.com/c/nlp-getting-started/overview)

```{r}

# This will create a "data" folder in the current directory and store the data in it

if (!file.exists("data")){
   dir.create("data")
}

trainurl <- "https://storage.googleapis.com/kagglesdsdata/competitions/17777/869809/train.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1602402299&Signature=bP6vq%2FzMH7wwZfHDce5GKr2WaTSDQZuEtqX%2FNRtX%2B5qA0m10jVcrDUsj555XRdeijGUnJ6HkY%2BtIgLpbIsgjrFC5UUimgqwbexpPi8VaL0gFOIAa88e01FMSMNlk7lBDkzDsHmLR1pYjqY5SCX4YXFCNMnuXYbU5D87cQHMxgDZn1BFz2dJgNtj1yoympLH4UMIfvICKj8SD3xAfSFU913i6qqvVRoX19Jlmw4ejJOoJO2wFLexoaQ6UlezBDavpUpZUaxp1YzM6Y4NHNvIuIslSz45DJuYtBlDwoHOaJtrMnggAohZqHPHoz51BI%2Fsr6ImCnqVVlM5Beme%2BLTCsEw%3D%3D&response-content-disposition=attachment%3B+filename%3Dtrain.csv"

trainfile <- "./data/train.csv"

testurl <- "https://storage.googleapis.com/kagglesdsdata/competitions/17777/869809/test.csv?GoogleAccessId=web-data@kaggle-161607.iam.gserviceaccount.com&Expires=1602402306&Signature=KUi2ioNjD0fmj%2BNFnndPkeVjMzaV8CwlG90ZwrwC1x6rdwS8Cxz5P7a%2BdEhhfqrWk5N7cirMTcekO21t2mSHJ0b0cyH5hhxZvMdT0is9Q5oQqC852MZ5DRGFWxHHRmyqiaoqTznNn5qp6TIoqGDqu7iQkLbejBVJx57ledBVQASqN%2BvQJNJNwMduqGKWdNKnjKuG0Q9B%2F830jZS5AxQHyiSlXqIG5j9QyotLJU0MjX%2BI920vfmj6seeG57ls9ENmb7Fd5VgWdwr0Wo%2BKLGizjszLL%2BfVMmW%2F4OZC%2FlOWgvn4LpH5NrTNwPTZGA5VLUhcnHpYHH2rtxsi0Ax3tEq6zQ%3D%3D&response-content-disposition=attachment%3B+filename%3Dtest.csv"

testfile <- "./data/test.csv"

if (!file.exists(trainfile)){
   download.file(trainurl, trainfile, method = "curl")
}

if (!file.exists(testfile)){
   download.file(testurl, testfile, method = "curl")
}

testing <- read.csv(testfile)
training <- read.csv(trainfile)
```

# Exploratory data analysis

It's now time to take a look at our data. The first thing we'll do is to convert both train and test sets to data tables to make future modifications easier. 

```{r}
training <- data.table(training)
testing <- data.table(testing)
```

For future data cleaning and processing, we'll combine the training and testing set into one complete set :

```{r}
comb <- data.table(rbind(training, mutate(testing, target = NA)))
```

We'll also store the testing set IDs which will be necessary for future submissions :

```{r}
test_id <- testing$id
```



## Dimensions

Let's have a look at the dimensions of our training set :

```{r}
dim(training)
```

## Exploring the response variable

Ou response variable is *target*, which takes the value 1 if a tweet is about a real disaster and 0 if not.

```{r}
ct <- c(nrow(filter(comb[!is.na(target), ], target == 0)), 
        nrow(filter(comb[!is.na(target), ], target == 1)))

ggplot(data = comb[!is.na(target), ], aes(x = factor(target), fill = target)) + 
   geom_bar(stat = "count") + theme(legend.position = "none") +
   labs(x = "Target (1 if disaster related tweet, 0 if not)", y = "Count")+
   ggtitle("Response variable repartition in the training set") +
   geom_text(stat = "count", aes(label = ..count..), vjust = -1) +
   coord_cartesian(ylim = c(0, 5000))
   
```

In our training set, the response variable's repartition is rather balanced, which means we have the possibility to built a very accurate model.

# Feature Engineering

## Text Length

```{r}
comb[, text_length := nchar(text), by = seq_len(nrow(comb))]
```

```{r}
ggplot(comb[!is.na(target), ], aes(x = factor(target), y = text_length, fill = target)) + geom_boxplot()
```

Disaster tweets seem to be a bit longer

## Number of capital letters

```{r}
comb[, ncaps := str_count(text, "[A-Z]"), by = seq_len(nrow(comb))]
```

## Number of links

```{r}
comb[, linkcount := str_count(text, "http"), by = seq_len(nrow(comb))]
```

## Number of punctuation

```{r}
comb[, punc_count := str_count(text, "[\\!\\?\\:\\.]"), by = seq_len(nrow(comb))]
```

## Number of hashtags

```{r}
comb[, hashtag_count := str_count(text, "#"), by = seq_len(nrow(comb))]
```

## Number of mentions

```{r}
comb[, mention_count := str_count(text, "@"), by = seq_len(nrow(comb))]
```


# Creating a DFM

## Tokenization

The first thing we will do with our training set is tokenization : we will split our lines of text into chunks of words. For instance, we want the sentence "This watch is Mike's" to become ["this" "watch" "is" "mike" "s"].

This is where the package quanteda comes in ; it can automate this process with its tokens() function. We will set the following parameters :

- Do not tokenize numbers
- Do not tokenize punctuation
- Do not tokenize isolated symbols such as dollar signs or hash
- Do not tokenize URLS
- Do not tokenize twitter words such as "rt" 
- Split hyphenated words

```{r}
comb_corpus <- corpus(comb$text)
comb_corpus <- stri_replace_all_regex(comb_corpus, "[\\p{p}\\p{S}]", "")
comb_corpus <- stri_replace_all_regex(comb_corpus, "http.*", "")
comb_tokens <- tokens(comb_corpus, what = "word", remove_numbers = T,
                       remove_punct = T, remove_symbols = T, split_hyphens = T,
                       remove_url = T, remove_twitter = T)
```


## Preprocessing the tokens

```{r}
comb_tokens <- tokens_tolower(comb_tokens)
comb_tokens <- tokens_select(comb_tokens, stopwords(), selection = "remove")
comb_tokens <- tokens_wordstem(comb_tokens)
```

## Creating the DFM 

```{r}
comb_dfm <- dfm(comb_tokens)
comb_dfm <- dfm_trim(comb_dfm, min_termfreq = 5)
```

We obtain our document frequency matrix, for which we trimmed down words that appear less than 5 times overall. 

We still end up with a very large matrix :

```{r}
dim(comb_dfm)
```

# Back to feature engineering : word matches

## Unigrams

### Most frequent unigrams

```{r}
dis_ss <- comb$target == 1 ; dis_ss[is.na(dis_ss)] <- F
nodis_ss <- comb$target == 0 ; nodis_ss[is.na(nodis_ss)] <- F
```

```{r}
dis_unidfm <- comb_dfm[dis_ss, ]
dis_unicount <- colSums(dis_unidfm)
dis_unicount <- dis_unicount[dis_unicount > 80]
dis_freq1gs <- names(dis_unicount)
```



```{r}
textplot_wordcloud(dis_unidfm, min.freq = 80, color = brewer.pal(10, "BrBG"))  
title("Most frequent words in disaster tweets", col.main = "grey14")
```

```{r}
nodis_unidfm <- comb_dfm[nodis_ss, ]
nodis_unicount <- colSums(nodis_unidfm)
nodis_unicount <- nodis_unicount[nodis_unicount > 80]
nodis_freq1gs <- names(nodis_unicount)
```

```{r}
textplot_wordcloud(nodis_unidfm, min.freq = 80, color = brewer.pal(10, "BrBG"))  
title("Most frequent words in non-disaster tweets", col.main = "grey14")
```
### Number of disaster or non-disaster words per tweet

```{r}
red_dis1g <- which(dis_freq1gs %in% nodis_freq1gs)
red_nodis1g <- which(nodis_freq1gs %in% dis_freq1gs)

dis_freq1gs <- dis_freq1gs[-red_dis1g]
nodis_freq1gs <- nodis_freq1gs[-red_nodis1g]
```

```{r}
comb[, nr_disasterWords := sum(str_count(text, dis_freq1gs)), by = seq_len(nrow(comb))]
comb[, nr_noDisasterWords := sum(str_count(text, nodis_freq1gs)), by = seq_len(nrow(comb))]
```

## Bigrams

### Most frequent bigrams :

```{r}
comb_bigrams <- tokens_ngrams(comb_tokens, n = 2)
comb_dfm2g <- dfm(comb_bigrams)
```

```{r}
dis_dfm2g <- comb_dfm2g[dis_ss, ]
dis_count2g <- colSums(dis_dfm2g)
dis_count2g <- dis_count2g[dis_count2g > 25]

textplot_wordcloud(dis_dfm2g, min.freq = 25, color = brewer.pal(10, "BrBG"))  
title("Most frequent bigrams (2Gs) in non-disaster tweets", col.main = "grey14")
```

```{r}
nodis_dfm2g <- comb_dfm2g[nodis_ss, ]
nodis_count2g <- colSums(nodis_dfm2g)
nodis_count2g <- nodis_count2g[nodis_count2g > 15]

textplot_wordcloud(nodis_dfm2g, min.freq = 15, color = brewer.pal(10, "BrBG"))  
title("Most frequent bigrams (2Gs) in non-disaster tweets", col.main = "grey14")
```

```{r}
dis_freq2gs <- names(dis_count2g)
nodis_freq2gs <- names(nodis_count2g)

red_dis2g <- which(dis_freq2gs %in% nodis_freq2gs)
red_nodis2g <- which(nodis_freq2gs %in% dis_freq2gs)

dis_freq2gs <- dis_freq2gs[-red_dis2g]
nodis_freq2gs <- nodis_freq2gs[-red_nodis2g]
```

```{r}
preprocessed_tokens <- list()

for (i in 1:length(comb_tokens)){
   rslt <- paste(comb_tokens[i], collapse = "_")
   preprocessed_tokens <- append(preprocessed_tokens, rslt)
}

```

```{r}
bigram_match <- function(textinput, bigram_list){
   num_output <- sum(str_count(textinput, bigram_list))
   num_output
}
```

```{r}
nr_disaster2gs <- sapply(preprocessed_tokens, bigram_match, dis_freq2gs)
nr_noDisaster2gs <- sapply(preprocessed_tokens, bigram_match, nodis_freq2gs)
```

```{r}
comb$nr_disaster2gs <- nr_disaster2gs
comb$nr_noDisaster2gs <- nr_noDisaster2gs
```

# SVD

Given the size of our DFM, we'll want to see which features provide the most variability, which means analyzing its singular value decomposition.

Let's find out how many singular vectors we need to capture 90% of the variance :

```{r, eval=F}
prep <- preProcess(comb_dfm, method = c("center", "scale", "pca"), thresh = 0.9)
```

```{r, echo=F}
load
```

```{r}
prep
```

Out of the `r dim(comb_dfm)[2]` features we have, we'll only keep the 1889 that provide the most variability. This SVD process can be automated by the **irlba** function by the package of the same name :

```{r, eval=F}
comb_svd <- irlba(t(comb_dfm), nv = 1889, maxit = 1000)
comb_topfeatures <- comb_svd$v
```

```{r, eval=F}
load
```

# Encoding the keyword variables

```{r}
comb[keyword == ""]$keyword <- "None"
```

```{r}
kw <- data.table(comb$keyword) ; names(kw) <- "keyword_"
ohe <- dummyVars(~ ., data = kw)
ohecat_kw <- predict(ohe, kw)
ohecat_kw <- data.table(ohecat_kw)
```

# Creating the complete data table

```{r}
comb_complete <- data.table(target = comb$target, 
                            ncaps = comb$ncaps,
                            text_length = comb$text_length,
                            linkcount = comb$linkcount,
                            hashtag_count = comb$hashtag_count,
                            mention_count = comb$mention_count,
                            punc_count = comb$mention_count,
                            nr_disasterWords = comb$nr_disasterWords,
                            nr_noDisasterWords = comb$nr_noDisasterWords,
                            nr_disaster2gs = comb$nr_disaster2gs,
                            nr_noDisaster2gs = comb$nr_noDisaster2gs,
                            ohecat_kw,
                            comb_topfeatures)
```


## splitting back to training and testing 

```{r}
training_range <- 1:nrow(training)
testing_range <- (nrow(training)+1):nrow(comb)
model_train <- comb_complete[training_range]
preproc_test <- comb_complete[testing_range]
```

# Model Building

## XGBoost

```{r, eval=F}

library(parallel)
library(doParallel)
cluster1 <- makeCluster(detectCores() - 2)
registerDoParallel(cluster1)

trctrl <- trainControl(method = "cv", number = 5, allowParallel = T, 
                       verboseIter = F)

xgbGrid <- expand.grid(nrounds = 750,  
                       max_depth = seq(1, 10, by = 1),
                       colsample_bytree = seq(0.2, 0.6, length.out = 5),
                       eta = seq(0.04, 0.7, length.out = 4),
                       gamma=0,
                       min_child_weight = seq(1, 5, by = 1),
                       subsample = 1
                      )
set.seed(69420)
modfit_xgbm <- caret::train(target ~ ., data = model_train,
                            method = "xgbTree", 
                            trControl = trctrl,
                            tuneGrid = xgbGrid, verbose = F)
stopCluster(cluster1)
registerDoSEQ()

```


Let's see what the best tune for the XGB model is :

```{r}
modfit_xgbm$bestTune
```

We set these as default hyperparameters :

```{r}
xgbparam <- list(objective = "binary:logistic",
                 booster = "gbtree",
                 max_depth = 6,
                 colsample_bytree = 0.5,
                 eta = 0.01,
                 gamma=0,
                 min_child_weight = 1,
                 subsample = 1
                )

```

Now we'll do another series of cross-validations, this time to figure out the best number of rounds :

```{r}
set.seed(1234)
xgb_cv <- xgb.cv(params = xgbparam, data = as.matrix(model_train), 
                 nrounds = 10000, nfold = 5, showsd = T, stratified = T, 
                 print_every_n = 15, early_stopping_rounds = 5, maximize = F, 
                 label = model_train$target)
```

we now know all the parameters for a perfect xgb model fit :

```{r}
dmat_train <- xgb.DMatrix(data = as.matrix(select(model_train, -target)), label = model_train$target)

```

```{r, eval=F}
modfit_xgbm2 <- xgb.train(dmat_train, params = xgbparam, nrounds = 20)
```

```{r}
imp_mat <- xgb.importance(feature_names = colnames(model_train), model = modfit_xgbm2)
xgb.ggplot.importance(importance_matrix = imp_mat[1:20], rel_to_first = T)
```

## Random Forest

```{r, eval=F}
library(parallel)
library(doParallel)
cluster1 <- makeCluster(detectCores() - 2)
registerDoParallel(cluster1)

set.seed(69420)
modfit_rf <- caret::train(target ~ ., data = model_train,
                            method = "rf", 
                            trControl = trctrl)
stopCluster(cluster1)
registerDoSEQ()
```

```{r}
imp_rf <- importance(modfit_rf$finalModel)
imp_rf <- data.table(variables = row.names(imp_rf), inc_mse = imp_rf[, 1])
imp_rf <- arrange(imp_rf, desc(inc_mse))
imp_rf <- imp_rf[1:20]

ggplot(data = imp_rf, 
       aes(x = reorder(variables, inc_mse), y = inc_mse, fill = inc_mse)) + 
   geom_bar(stat = "identity") +
   theme(legend.position = "none")+
   labs(x = "Features", 
        y = "% increase in MSE if variable is shuffled")+
   coord_flip()
```
